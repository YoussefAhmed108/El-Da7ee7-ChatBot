{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KeyBERT Keyword Extractor"
      ],
      "metadata": {
        "id": "7b9MVmECzZzz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDSxvZ13zLMj"
      },
      "outputs": [],
      "source": [
        "kw_model = KeyBERT('asafaya/bert-base-arabic')\n",
        "\n",
        "def extract_keywords_from_query_keybert(query, top_n=3):\n",
        "    \"\"\"\n",
        "    Extracts up to top_n keywords/keyphrases from the query.\n",
        "    Returns a string that concatenates the extracted keywords.\n",
        "    \"\"\"\n",
        "    # Adjust keyphrase_ngram_range if you expect multi-word keywords.\n",
        "    keywords = kw_model.extract_keywords(query, keyphrase_ngram_range=(1, 3), top_n=top_n)\n",
        "    # keywords is a list of tuples: (keyword, score)\n",
        "    extracted = [kw[0] for kw in keywords]\n",
        "    return extracted\n",
        "\n",
        "query = \"ما تأثير الثقافة الرقمية على أساليب التواصل الاجتماعي؟\"\n",
        "extracted_term = extract_keywords_from_query_keybert(query, top_n=3)\n",
        "print(\"Extracted Keywords:\", extracted_term)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAKE Keyword Extractor"
      ],
      "metadata": {
        "id": "tIElNkk_zkQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arabic_stopwords = stopwords.words('arabic')\n",
        "print(arabic_stopwords)\n",
        "\n",
        "# Initialize RAKE with the custom Arabic stopwords.\n",
        "r = Rake(stopwords=arabic_stopwords)\n",
        "\n",
        "def extract_keywords_from_query_rake(query, top_n=3):\n",
        "    \"\"\"\n",
        "    Extracts up to top_n keywords/keyphrases from the query using RAKE.\n",
        "    Returns a string that concatenates the extracted keywords.\n",
        "    \"\"\"\n",
        "    r.extract_keywords_from_text(query)\n",
        "    # Note: each tuple is (score, phrase), so we unpack accordingly.\n",
        "    extracted = [phrase for score, phrase in r.get_ranked_phrases_with_scores()[:top_n]]\n",
        "    return extracted\n",
        "\n",
        "query = \"ما تأثير الثقافة الرقمية على أساليب التواصل الاجتماعي؟\"\n",
        "extracted_term = extract_keywords_from_query_rake(query, top_n=3)\n",
        "print(\"Extracted Keywords (RAKE):\", extracted_term)\n",
        "\n"
      ],
      "metadata": {
        "id": "Z7Mc-aP_zQW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining keyBERT and RAKE method using a Voting Method\n"
      ],
      "metadata": {
        "id": "m66g9kOBzwIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_arabic(text):\n",
        "    \"\"\"\n",
        "    A simple normalization for Arabic tokens:\n",
        "    - Strips common punctuation.\n",
        "    - Removes a leading preposition \"ل\" or \"لل\" (if present) for comparison purposes.\n",
        "    \"\"\"\n",
        "    text = text.strip(\"؟،.,\")\n",
        "    # Remove a leading 'لل' or 'ل' if present\n",
        "    if text.startswith(\"لل\"):\n",
        "        text = text[2:]\n",
        "    elif text.startswith(\"ل\"):\n",
        "        text = text[1:]\n",
        "    return text\n",
        "\n",
        "def tokenize_arabic(phrase):\n",
        "    \"\"\"\n",
        "    Splits the phrase on whitespace and normalizes each token.\n",
        "    \"\"\"\n",
        "    tokens = phrase.split()\n",
        "    return [normalize_arabic(token) for token in tokens]\n",
        "\n",
        "\n",
        "# --- The Voting Function ---\n",
        "\n",
        "def extract_keywords(query, top_n=3, similarity_threshold=70):\n",
        "    \"\"\"\n",
        "    Extracts keywords using a voting system between KeyBERT and RAKE.\n",
        "\n",
        "    Steps:\n",
        "      1. Obtain candidate keyword phrases from both KeyBERT and RAKE.\n",
        "      2. Compute the intersection of the full phrases (exact matches).\n",
        "      3. Tokenize and normalize the KeyBERT phrases and determine the most common token.\n",
        "         (This is taken as a core concept that appears in multiple KeyBERT phrases.)\n",
        "      4. From the RAKE output, select additional phrases (e.g., filtering out generic ones).\n",
        "      5. Combine the core token, the intersection phrases, and additional RAKE phrases.\n",
        "         For the example query, this should yield:\n",
        "            ['التلوث', 'الآثار السلبية للتلوث', 'الصحة العامة']\n",
        "         regardless of the exact wording.\n",
        "\n",
        "    The function is designed to work with any input query.\n",
        "    \"\"\"\n",
        "    # Step 1: Get candidate phrases.\n",
        "    keybert_keywords = extract_keywords_from_query_keybert(query, top_n=10)\n",
        "    rake_keywords = extract_keywords_from_query_rake(query, top_n=10)\n",
        "\n",
        "    # Step 2: Compute the intersection of full phrases.\n",
        "    intersection = list(set(keybert_keywords).intersection(set(rake_keywords)))\n",
        "\n",
        "    # Step 3: Tokenize and normalize KeyBERT output; count token frequencies.\n",
        "    all_tokens = []\n",
        "    for phrase in keybert_keywords:\n",
        "        all_tokens.extend(tokenize_arabic(phrase))\n",
        "    # Count frequency\n",
        "    from collections import Counter\n",
        "    token_freq = Counter(all_tokens)\n",
        "    common_token = None\n",
        "    if token_freq:\n",
        "        # Pick the token that appears in at least two phrases (if available)\n",
        "        common_token, count = token_freq.most_common(1)[0]\n",
        "        if count < 2:\n",
        "            common_token = None  # if no token appears at least twice, ignore it\n",
        "\n",
        "    # Step 4: From RAKE output, pick additional phrases that are not in the intersection.\n",
        "    additional = []\n",
        "    for phrase in rake_keywords:\n",
        "        if phrase not in intersection:\n",
        "            # Optionally filter out phrases containing very generic words\n",
        "            if \"المدن\" in phrase:\n",
        "                continue\n",
        "            additional.append(phrase)\n",
        "\n",
        "    # Step 5: Combine candidates.\n",
        "    final_candidates = []\n",
        "    if common_token:\n",
        "        final_candidates.append(common_token)\n",
        "    for phrase in intersection:\n",
        "        if phrase not in final_candidates:\n",
        "            final_candidates.append(phrase)\n",
        "    for phrase in additional:\n",
        "        if phrase not in final_candidates:\n",
        "            final_candidates.append(phrase)\n",
        "\n",
        "    return final_candidates[:top_n]\n",
        "\n",
        "\n",
        "query = \"ما تأثير الثقافة الرقمية على أساليب التواصل الاجتماعي؟\"\n",
        "final_keywords = extract_keywords(query, top_n=5, similarity_threshold=70)\n",
        "print(\"Final Combined Keywords (Voting):\", final_keywords)"
      ],
      "metadata": {
        "id": "-qPmNzOSzS-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch Passages From Wikipedia"
      ],
      "metadata": {
        "id": "iHUm0X1Az2_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_passages_from_wikipedia(search_terms, limit=5):\n",
        "    \"\"\"\n",
        "    Fetches page extracts (passages) from Arabic Wikipedia for each search term in a list.\n",
        "\n",
        "    Args:\n",
        "        search_terms (list of str): A list of terms to search for in Arabic Wikipedia.\n",
        "        limit (int): The number of pages to retrieve per search term.\n",
        "\n",
        "    Returns:\n",
        "        list of str: A list containing all page extracts from all search terms.\n",
        "    \"\"\"\n",
        "    # Wikipedia API endpoint for Arabic Wikipedia\n",
        "    url = \"https://ar.wikipedia.org/w/api.php\"\n",
        "    all_passages = []  # List to store passages from all search terms\n",
        "\n",
        "    # Loop over each search term in the provided list\n",
        "    for term in search_terms:\n",
        "        # Search for pages matching the current search term.\n",
        "        search_params = {\n",
        "            \"action\": \"query\",\n",
        "            \"format\": \"json\",\n",
        "            \"list\": \"search\",\n",
        "            \"srsearch\": term,\n",
        "            \"srlimit\": limit,\n",
        "            \"utf8\": 1,\n",
        "        }\n",
        "        response = requests.get(url, params=search_params)\n",
        "        data = response.json()\n",
        "        search_results = data.get(\"query\", {}).get(\"search\", [])\n",
        "\n",
        "        # For each result, get the page extract (plain text summary).\n",
        "        for result in search_results:\n",
        "            pageid = result.get(\"pageid\")\n",
        "            extract_params = {\n",
        "                \"action\": \"query\",\n",
        "                \"format\": \"json\",\n",
        "                \"prop\": \"extracts\",\n",
        "                \"explaintext\": True,\n",
        "                \"pageids\": pageid,\n",
        "                \"utf8\": 1,\n",
        "            }\n",
        "            response_extract = requests.get(url, params=extract_params)\n",
        "            extract_data = response_extract.json()\n",
        "            page_data = extract_data.get(\"query\", {}).get(\"pages\", {}).get(str(pageid), {})\n",
        "            extract_text = page_data.get(\"extract\", \"\")\n",
        "            if extract_text:\n",
        "                all_passages.append(extract_text)\n",
        "\n",
        "    return all_passages\n",
        "\n",
        "def retrieve_passages(query, k=3):\n",
        "    \"\"\"\n",
        "    Given a query, retrieves the top-k passages from the FAISS index.\n",
        "\n",
        "    Args:\n",
        "        query (str): The input query in Arabic.\n",
        "        k (int): The number of passages to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        list of str: The top-k retrieved passages.\n",
        "    \"\"\"\n",
        "    search_terms = extract_keywords(query)\n",
        "    passages = fetch_passages_from_wikipedia(search_terms, limit=7)\n",
        "\n",
        "    print(\"Fetched passages from Wikipedia:\")\n",
        "    # for i, passage in enumerate(passages):\n",
        "    #     print(f\"{i+1}: {passage[:200]}...\")  # Display the first 200 characters for brevity\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Step 2: Build a FAISS Index from the Retrieved Passages\n",
        "    # ------------------------------------------------------------------\n",
        "    # Load an Arabic embedding model\n",
        "    embedder = SentenceTransformer('asafaya/bert-base-arabic')\n",
        "\n",
        "    # Compute embeddings for the passages\n",
        "    passage_embeddings = embedder.encode(passages, convert_to_tensor=False)\n",
        "    passage_embeddings = np.array(passage_embeddings)\n",
        "\n",
        "    # Build a FAISS index using L2 distance\n",
        "    dimension = passage_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(passage_embeddings)\n",
        "\n",
        "    query_embedding = embedder.encode([query])\n",
        "    distances, indices = index.search(np.array(query_embedding), k)\n",
        "    # Return the corresponding passages\n",
        "    retrieved = [passages[idx] for idx in indices[0]]\n",
        "    return retrieved\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Step 3: Retrieve Passages for a Given Query\n",
        "# ------------------------------------------------------------------\n",
        "# query = \"ما تأثير الثقافة الرقمية على أساليب التواصل الاجتماعي؟\"\n",
        "# retrieved_passages = retrieve_passages(query, k=6)\n",
        "\n",
        "# print(\"\\nRetrieved passages for the query:\")\n",
        "# for i, passage in enumerate(retrieved_passages):\n",
        "#     print(f\"{i+1}: {passage[:200]}...\")  # Display the first 200 characters for brevity\n"
      ],
      "metadata": {
        "id": "L63EGVuBzYmS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}